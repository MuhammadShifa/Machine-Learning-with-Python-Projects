{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Python Data Preprocessing Techniques.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP39JrBdWvJmfi/CR5aINAZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5EScLH9_t1uW"},"source":["#### Python Data Preprocessing Techniques for Machine Learning Algorithms\r\n","\r\n","Machine Learning algorithms don’t work so well with processing raw data. Before we can feed such data to an ML algorithm, we must preprocess it. In other words, we must apply some transformations on it. With data preprocessing, we convert raw data into a clean data set.\r\n","\r\n","let't discuss some methods of preprocesssing...\r\n","\r\n","@author: Muhammad Shifa"]},{"cell_type":"code","metadata":{"id":"faueocwLc_OY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613913342136,"user_tz":-300,"elapsed":1942,"user":{"displayName":"Muhammad Shifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN68vWJissbgbLe7JfYv-heHmocSZllWw0OyRfdQ=s64","userId":"04523674318540349393"}},"outputId":"9905b2ea-2c69-4394-ac84-74bea28ba0ac"},"source":["# import necessary packages\n","\n","import numpy as np\n","import scipy as sc\n","import pandas as pd\n","import sklearn as sk\n","\n","#Take any dataset you want\n","dataFframe = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",sep =';')\n","print(dataFframe)\n","#Return a Numpy representation of the DataFrame.\n","array = dataFframe.values\n","print(array)\n","#Separating data into input and output components\n","x = array[:,0:8]\n","y = array[:,8]\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["      fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n","0               7.4             0.700         0.00  ...       0.56      9.4        5\n","1               7.8             0.880         0.00  ...       0.68      9.8        5\n","2               7.8             0.760         0.04  ...       0.65      9.8        5\n","3              11.2             0.280         0.56  ...       0.58      9.8        6\n","4               7.4             0.700         0.00  ...       0.56      9.4        5\n","...             ...               ...          ...  ...        ...      ...      ...\n","1594            6.2             0.600         0.08  ...       0.58     10.5        5\n","1595            5.9             0.550         0.10  ...       0.76     11.2        6\n","1596            6.3             0.510         0.13  ...       0.75     11.0        6\n","1597            5.9             0.645         0.12  ...       0.71     10.2        5\n","1598            6.0             0.310         0.47  ...       0.66     11.0        6\n","\n","[1599 rows x 12 columns]\n","[[ 7.4    0.7    0.    ...  0.56   9.4    5.   ]\n"," [ 7.8    0.88   0.    ...  0.68   9.8    5.   ]\n"," [ 7.8    0.76   0.04  ...  0.65   9.8    5.   ]\n"," ...\n"," [ 6.3    0.51   0.13  ...  0.75  11.     6.   ]\n"," [ 5.9    0.645  0.12  ...  0.71  10.2    5.   ]\n"," [ 6.     0.31   0.47  ...  0.66  11.     6.   ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dGyTcFbTvVwQ"},"source":["###a. Rescaling Data\r\n","\r\n","For data with attributes of varying scales, we can rescale attributes to possess the same scale. We rescale attributes into the range 0 to 1 and call it normalization. We use the MinMaxScaler class from scikit-learn. Let’s see an example."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZmaLq86au8ET","executionInfo":{"status":"ok","timestamp":1613913559909,"user_tz":-300,"elapsed":963,"user":{"displayName":"Muhammad Shifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN68vWJissbgbLe7JfYv-heHmocSZllWw0OyRfdQ=s64","userId":"04523674318540349393"}},"outputId":"0c6e197c-7305-41cc-9481-67f4e416d548"},"source":["from sklearn.preprocessing import MinMaxScaler\r\n","\r\n","scaler = MinMaxScaler(feature_range=(0,1))\r\n","reScaledX = scaler.fit_transform(x)\r\n","np.set_printoptions(precision=3)\r\n","print(reScaledX)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[[0.248 0.397 0.    ... 0.141 0.099 0.568]\n"," [0.283 0.521 0.    ... 0.338 0.216 0.494]\n"," [0.283 0.438 0.04  ... 0.197 0.17  0.509]\n"," ...\n"," [0.15  0.267 0.13  ... 0.394 0.12  0.416]\n"," [0.115 0.36  0.12  ... 0.437 0.134 0.396]\n"," [0.124 0.13  0.47  ... 0.239 0.127 0.398]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hqchuILnvy3A"},"source":["This gives us values between 0 and 1. Rescaling data proves of use with neural networks, optimization algorithms and those that use distance measures like k-nearest neighbors and weight inputs like regression."]},{"cell_type":"markdown","metadata":{"id":"C0daTe2bwA0Z"},"source":["###b. Standardizing Data\r\n","\r\n","With standardizing, we can take attributes with a Gaussian distribution and different means and standard deviations and transform them into a standard Gaussian distribution with a mean of 0 and a standard deviation of 1. For this, we use the StandardScaler class. Let’s take an example."]},{"cell_type":"code","metadata":{"id":"caO-iUDrbGm3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613913716870,"user_tz":-300,"elapsed":911,"user":{"displayName":"Muhammad Shifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN68vWJissbgbLe7JfYv-heHmocSZllWw0OyRfdQ=s64","userId":"04523674318540349393"}},"outputId":"4836ad26-3625-4994-d78b-930147f2f0df"},"source":["from sklearn.preprocessing import StandardScaler\n","#Standardizing Data\n","\n","scaler = StandardScaler().fit(x)\n","scaledX = scaler.transform(x)\n","print(scaledX)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[[-0.528  0.962 -1.391 ... -0.466 -0.379  0.558]\n"," [-0.299  1.967 -1.391 ...  0.873  0.624  0.028]\n"," [-0.299  1.297 -1.186 ... -0.084  0.229  0.134]\n"," ...\n"," [-1.16  -0.1   -0.724 ...  1.255 -0.197 -0.534]\n"," [-1.39   0.655 -0.775 ...  1.542 -0.075 -0.677]\n"," [-1.333 -1.217  1.022 ...  0.203 -0.136 -0.666]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QZku44YMwWZA"},"source":["###c. Normalizing Data\r\n","In this task, we rescale each observation to a length of 1 (a unit norm). For this, we use the Normalizer class. Let’s take an example."]},{"cell_type":"code","metadata":{"id":"5XDsD6-DcPxL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613913766316,"user_tz":-300,"elapsed":908,"user":{"displayName":"Muhammad Shifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN68vWJissbgbLe7JfYv-heHmocSZllWw0OyRfdQ=s64","userId":"04523674318540349393"}},"outputId":"e40345d0-2a1a-4e50-af81-e843ef40d71d"},"source":["from sklearn.preprocessing import Normalizer\n","#Normalizing Data\n","scaler = Normalizer().fit(x)\n","normalizedX = scaler.transform(x)\n","print(normalizedX)\n","normalizedX[0:5,:6]"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[[2.024e-01 1.914e-02 0.000e+00 ... 3.008e-01 9.299e-01 2.729e-02]\n"," [1.083e-01 1.222e-02 0.000e+00 ... 3.472e-01 9.306e-01 1.385e-02]\n"," [1.377e-01 1.342e-02 7.061e-04 ... 2.648e-01 9.533e-01 1.760e-02]\n"," ...\n"," [1.263e-01 1.023e-02 2.607e-03 ... 5.815e-01 8.020e-01 1.997e-02]\n"," [1.077e-01 1.178e-02 2.191e-03 ... 5.842e-01 8.033e-01 1.817e-02]\n"," [1.298e-01 6.704e-03 1.016e-02 ... 3.893e-01 9.083e-01 2.153e-02]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[0.202, 0.019, 0.   , 0.052, 0.002, 0.301],\n","       [0.108, 0.012, 0.   , 0.036, 0.001, 0.347],\n","       [0.138, 0.013, 0.001, 0.041, 0.002, 0.265],\n","       [0.177, 0.004, 0.009, 0.03 , 0.001, 0.268],\n","       [0.202, 0.019, 0.   , 0.052, 0.002, 0.301]])"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"XV0VhHECwigG"},"source":["###d. Binarizing Data\r\n","Using a binary threshold, it is possible to transform our data by marking the values above it 1 and those equal to or below it, 0. For this purpose, we use the Binarizer class. Let’s take an example."]},{"cell_type":"code","metadata":{"id":"AJt7IOuafqJq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613913888729,"user_tz":-300,"elapsed":964,"user":{"displayName":"Muhammad Shifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN68vWJissbgbLe7JfYv-heHmocSZllWw0OyRfdQ=s64","userId":"04523674318540349393"}},"outputId":"c2cf5dab-e513-4b81-f2a7-2dc55b90ec12"},"source":["from sklearn.preprocessing import Binarizer\n"," #Binarizing Data\n","scaler = Binarizer(threshold = 0.1).fit(x)\n","binarizedX = scaler.transform(x)\n","print(binarizedX)\n","binarizedX[0:5, : ]"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[[1. 1. 0. ... 1. 1. 1.]\n"," [1. 1. 0. ... 1. 1. 1.]\n"," [1. 1. 0. ... 1. 1. 1.]\n"," ...\n"," [1. 1. 1. ... 1. 1. 1.]\n"," [1. 1. 1. ... 1. 1. 1.]\n"," [1. 1. 1. ... 1. 1. 1.]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[1., 1., 0., 1., 0., 1., 1., 1.],\n","       [1., 1., 0., 1., 0., 1., 1., 1.],\n","       [1., 1., 0., 1., 0., 1., 1., 1.],\n","       [1., 1., 1., 1., 0., 1., 1., 1.],\n","       [1., 1., 0., 1., 0., 1., 1., 1.]])"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"FmC-LJJyw6Vn"},"source":["This marks 0 over all values equal to or less than 0, and marks 1 over the rest. When you want to turn probabilities into crisp values, this functionality comes handy."]},{"cell_type":"markdown","metadata":{"id":"auRjioZsxGL7"},"source":["###e. Mean Removal\r\n","\r\n","We can remove the mean from each feature to center it on zero."]},{"cell_type":"code","metadata":{"id":"JOOyeR7nhpJp","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1603614507002,"user_tz":-300,"elapsed":996,"user":{"displayName":"Muhammad Shifa","photoUrl":"https://lh4.googleusercontent.com/-9TFCQJoYLIE/AAAAAAAAAAI/AAAAAAAAABU/gFsmvpz0AZ4/s64/photo.jpg","userId":"04523674318540349393"}},"outputId":"c183c40f-350f-467e-fe7a-9e5249dbf233"},"source":["from sklearn.preprocessing import scale\n","\n","#Mean Removal\n","data_standardized = scale(dataFframe)\n","data_mean = data_standardized.mean(axis = 0)\n","data_std = data_standardized.std()\n","print(data_mean)\n","print(data_std)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[ 3.555e-16  1.733e-16 -8.887e-17 -1.244e-16  3.910e-16 -6.221e-17\n","  4.444e-17  2.364e-14  2.862e-15  6.754e-16  1.066e-16  8.887e-17]\n","1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8ftS85-exTlk"},"source":["###f. One Hot Encoding\r\n","When dealing with few and scattered numerical values, we may not need to store these. Then, we can perform One Hot Encoding. For k distinct values, we can transform the feature into a k-dimensional vector with one value of 1 and 0 as the rest values."]},{"cell_type":"code","metadata":{"id":"BxRxlo_Vji5j"},"source":["#One Hot Encoding\n","\n","from sklearn.preprocessing import OneHotEncoder\n","\n","encoder = OneHotEncoder()\n","encodedX = encoder.fit(x)\n","encodedX=encoder.transform(x)\n","print(encodedX)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8Har4Zfxghk"},"source":["###g. Label Encoding\r\n","Some labels can be words or numbers. Usually, training data is labelled with words to make it readable. Label encoding converts word labels into numbers to let algorithms work on them. Let’s take an example."]},{"cell_type":"code","metadata":{"id":"r0vIvM6qmezS"},"source":["#Label Encoding\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","label_encoder = LabelEncoder()\n","input_classes = ['Havells','Philips','Syska','Eveready','Lloyd']\n","label_encoder.fit(input_classes)\n","for i,item in enumerate(label_encoder.classes_):\n","  print(item,'-->',i)"],"execution_count":null,"outputs":[]}]}